{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is ment to be a quick start quide. It includes tutorials to get started with `SCAN` model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCAN  : Semantic Clustering by Adopting Nearest Neighbors\n",
    "\n",
    "SCAN is an unsupervised approach which claims to achieve state of the art performance in image classification without using labels. The algorithm is divided into three stages lets discuss what each does in brief. \n",
    "\n",
    "The 3 parts which make up the algorithm are\n",
    "\n",
    "1. pretext : The Idea of pretext task comes from Representational learning where feature representation is learned solely from the images. This is done my `minimizing the distance between a image and its augmentation`. Since the neural net used has a `limited capacity it tries to remember only the important characteristics which help classify that image.`\n",
    "\n",
    "2. SCAN : This models learns on the weights of the pretext models and outputs a probablilty distributions for C classes where C in our case is the no. of clusters. Hence the output of this stage is similiar to that of a mutli-class classifier.\n",
    "\n",
    "3. Selflabel : This is a fine tuning step. The model used for selflabel and scan are same. \n",
    "\n",
    "Read <a hre = \"https://clivefernandes777.medium.com/scan-lets-classify-images-without-labels-e31388da9cfd\">my blogs </a> for more details \n",
    "\n",
    "\n",
    "Below Image depicts working of the model on the cifar-10 dataset.\n",
    "\n",
    "\n",
    "![](images/pipeline.png)\n",
    "\n",
    "\n",
    "The following <a href ='https://www.youtube.com/watch?v=hQEnzdLkPj4&feature=youtu.be' >youtube video</a> can is a good place if you want to understand the inner working of the mode. \n",
    "\n",
    "\n",
    "Link to the paper: https://arxiv.org/abs/2005.12320\n",
    "\n",
    "\n",
    "Link to the original Repo : https://github.com/wvangansbeke/Unsupervised-Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Folders and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/\n",
      "data/\n",
      "images/\n",
      "LandCover/\n",
      "losses/\n",
      "models/\n",
      "scan_no_validation/\n",
      "tutorial/\n",
      "utils/\n",
      "Weights/\n"
     ]
    }
   ],
   "source": [
    "!ls -d */"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `scan_no_validation` : SCAN repo. with validation code removed. In practical we wont have labels if we are using unsupervised approach\n",
    "2. `configs` : The configs folder contains 3 folders pretext,scan,selflabel one per phase. In this folder config files for cifar_10,eurosat and keyna_153x153 are included. \n",
    "\n",
    "3. `data` : This folder holds the data pipelines. \n",
    "\n",
    "4. `losses` : losse functions for each model. \n",
    "\n",
    "5. `models` : model defination. we are using the resnet-18 model  as the base model for all 3 parts. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan.py\n",
      "selflabel.py\n",
      "setup.py\n",
      "simclr.py\n"
     ]
    }
   ],
   "source": [
    "!ls -f ^s*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. simclr.py : This file is used to train the pretext model.It accepts 2 command line params. The env to work in and the config.yml file. \n",
    "eg.\n",
    "\n",
    "`!python pretext.py --config_env configs/env.yml --config_exp configs/pretext/simclr_kenya_custom_augmentation.yml`\n",
    "\n",
    "2. scan.py : This file is used to train the scan model. Inputs same as pretext but the config file is different. \n",
    "3. selflabel.py : This files trains the self-label model (fine tunes  the scan model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials\n",
    "\n",
    "## Copy the notebooks into the root folder otherwise there will be import error\n",
    "\n",
    "This folder contains 2 jupyter notebooks one  trained  on `cifar_10` dataset and the other to train on  `euroSat` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCAN Eurosat.ipynb\n",
      "SCAN_cifar10 _85.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar10\n",
    "       The Model was trained on the cifar10 dataset to check the claims of the authors. Run the **SCAN_cifar10_85** notebook to train on the cifar10 dataset. We dont need to download the dataset the there are scripts to take care of it.\n",
    "       \n",
    "## Eurosat \n",
    "       This dataset was quite similar to the our usecase  with 10 classes and would act as a fact check. The data needs to be downloaded. The final model accuracy is 92% on  which quite good.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Results on euroSat\n",
    "![](images/euroSAT_Result.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two plots on top represent the image representation of eurosat dataset plotted in 2d using PCA. Each color represents different classes can view the legend for more details. The plot of the left Side Represents the True labels and the one on the right are the predictions using the scan algorithm. As one can see the two images are nearly identical the model gets a 92% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating : Model Weights\n",
    "\n",
    "Model weights are saved in the Weights/euroSAT folder. We have 3 models (pretext,scan,selflabel) but for validation we only require one scan or selflabel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Training Hack` \n",
    "\n",
    "To train the model from a scratch a quick way or hack is to train the pretext model for some epochs (this is done as the weights are needed to start training scan model). The main part is the topk-train-neighbours located in the Weights/euroSAT/pretext/. This serve as the labels for the scan model. Since I have already trained the pretext model if all the configs are kept intact the scan model should produce similar results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Kenya Images \n",
    "\n",
    "\n",
    "## DataSet Description \n",
    "The dataset consists of 259 sentinel 2 10m resolution tif images.The image size is 153 x 153, to train the scan model RGB channels were extracted from the tiff images and the images were saved in .jpeg format.\n",
    "\n",
    "`REMARK` : At the end when mapping the clusters to the map. The tiff images were required as they contain the bounds (lat,lan). Instead of that saving the lat,lan of each image during the jepg conversion will be a better approach.\n",
    "\n",
    "\n",
    "# Results: \n",
    "\n",
    "1. The model has been trained for clusters C = 5,6,8. The aim of this clustering step was to get the Pseudo Labels. \n",
    "2. Since we didnt have labels here the original scan repo had to be modified to remove all the valdiation part. The   modified repo is saved in the folder `scan_with_no_validation`.\n",
    "3. The Results are stored in the Kenya_Cluster_Images folder they contain the cluster images, .csv files containing the lat,lan per cluster.\n",
    "4. The notebooks are stored in the Kenya NoteBooks Folder. lets see more abt. them.\n",
    "5. To train run the notebooks move them to the scan_with_no_validation\n",
    "6. All the config files are present there in the configs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kenya_map_plots.ipynb\n",
      "SCAN_(Kenya)_5_Clusters.ipynb\n",
      "SCAN_(Kenya)_6_Clusters.ipynb\n",
      "SCAN_(Kenya)_8_Clusters.ipynb\n",
      "SCAN_DataPipeline_Augmentation_keep.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls 'Kenya Notebooks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they model has been trained for cluster C = (5,6,8) we have one notebook for each. A few things to note down here are\n",
    "for cluster C = (5,8) the parametres for the config files are similar to the original scan repo. only chnaging the img_size = 153. \n",
    "\n",
    "\n",
    "For Cluster C = 6. Different augmentations were tried results can be found in the `SCAN_DataPipeline_Augmentation_keep` notebook. Now lets look at the cluster results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kenya Dataset Contininig 259 images \n",
    "\n",
    "![](images/all_before_attack_images.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster images for C = 5\n",
    "![](images/Cluster_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 6 (custom augmentation)  [Selected Model]\n",
    "![](images/Cluster_6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster 8\n",
    "![](images/Cluster_8.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the images are saved in the `Kenya_Cluster_Images` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Scatter Plots\n",
    "\n",
    "The following plot was obtained by first using PCA to get the (x,y) cordinates for each image. Then the images and the assigned cluster labels are plotted on the coordinates.\n",
    "\n",
    "![](images/pre_text_kmeans_cls_6.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Results: Custom Augmentations are saved in the Kenya_Cluster_Images/final_output_6\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Custom SCAN Model.\n",
    "\n",
    "Things to consider.\n",
    "\n",
    "\n",
    "1. config fles \n",
    "2. creating a dataset class \n",
    "3. modifying the common_config.py file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config Files \n",
    "\n",
    "Each stage (pretext,scan,self-label) have seperate files saved in the config folder. Here are some fields to consider when training on custom dataset.\n",
    "\n",
    "\n",
    "The config files can be divided into 3 sections \n",
    "\n",
    "1. Augmentations \n",
    "2. Optimizers \n",
    "3. Model and Dataset info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and dataset info.\n",
    "\n",
    "```yml \n",
    "\n",
    "# Setup\n",
    "setup: simclr\n",
    "\n",
    "train_dir: KenyaModel_5CLS\n",
    "\n",
    "# Model\n",
    "backbone: resnet18\n",
    "model_kwargs:\n",
    "   head: mlp\n",
    "   features_dim: 128\n",
    "\n",
    "# Dataset\n",
    "train_db_name: KenyaDataset\n",
    "num_classes: 5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the above file this is just a part of the config file. Lets understand what is means. \n",
    "\n",
    "\n",
    "1. `step` : This key represents the model being trained simclr is the approach used to train pretext model for the scan and self-label this will be replaced with their respective names. \n",
    "\n",
    "2. `train_dir` : path to the output folder. If the folder doesn't exist it will be created. Within the folder the pretext,scan,self-label will be stored. \n",
    "\n",
    "3. `backbone` : Model architecture to use. \n",
    "\n",
    "4. `feature_dim` : output size of the pretext model. \n",
    "\n",
    "5. `train_db_name` : name of the dataset class\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers: \n",
    "\n",
    "1. For the pretext model : Use only SDG algorithm as used in the original repo.\n",
    "2. For scan and self-label : Same optimizer is used for both for smaller images adam is used but for larger image size SGD works better. \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenataions \n",
    "\n",
    "For smaller images of size 32 or 64 scan and self-label model use augmentations proposed by the authors.\n",
    "```yml \n",
    "# Transformations\n",
    "augmentation_strategy: ours \n",
    "augmentation_kwargs:\n",
    "   crop_size: 32\n",
    "   normalize:\n",
    "      mean: [0.4914, 0.4822, 0.4465]\n",
    "      std: [0.2023, 0.1994, 0.2010]\n",
    "   num_strong_augs: 4\n",
    "   cutout_kwargs:\n",
    "     n_holes: 1\n",
    "     length: 16\n",
    "     random: True\n",
    "\n",
    "transformation_kwargs:\n",
    "   crop_size: 32\n",
    "   normalize:\n",
    "      mean: [0.4914, 0.4822, 0.4465]\n",
    "      std: [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "```\n",
    "\n",
    "1. augmentation_strategy : This key specifies which strategy is being used, followed by the args. On top the training augmentations are set and at bottom validation arguments. While the model is in inferene the validation args are used. These remain unchnaged for all 3 models. \n",
    "\n",
    "\n",
    "\n",
    "If using the default config the param to change is the `crop_size` set it to ur image size for kenya 153 \n",
    "\n",
    "\n",
    "**It is also notice that for larger image size eg (224 x 224) the authors have resulted in using same augmentation as the pretext for all 3 stages.**\n",
    "\n",
    "```yml\n",
    "\n",
    "# Transformations\n",
    "augmentation_strategy: simclr \n",
    "augmentation_kwargs:\n",
    "   random_resized_crop:\n",
    "      size: 153\n",
    "      scale: [0.2, 1.0]\n",
    "   color_jitter_random_apply:\n",
    "      p: 0.8\n",
    "   color_jitter:\n",
    "      brightness: 0.4\n",
    "      contrast: 0.4\n",
    "      saturation: 0.4\n",
    "      hue: 0.1\n",
    "   random_grayscale: \n",
    "      p: 0.2\n",
    "   normalize:\n",
    "      mean: [0.4914, 0.4822, 0.4465]\n",
    "      std: [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "transformation_kwargs:\n",
    "   crop_size: 153\n",
    "   normalize:\n",
    "      mean: [0.4914, 0.4822, 0.4465]\n",
    "      std: [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "These were the final custom augmentations used for training the final model. \n",
    "\n",
    "```yml\n",
    "augmentation_strategy: custom_aug\n",
    "augmentation_kwargs:\n",
    "   random_resized_crop:\n",
    "      size: 153\n",
    "      scale: [0.5, 1.0]\n",
    "      ratio: [0.75,1.3333]\n",
    "   color_jitter_random_apply:\n",
    "      p: 0.8\n",
    "   color_jitter:\n",
    "      brightness: [0.5,2]\n",
    "      contrast: [2,7]\n",
    "      saturation: 0\n",
    "      hue: 0\n",
    "   normalize:\n",
    "      mean: [0.4049, 0.3090, 0.2831]\n",
    "      std: [0.2702, 0.1792, 0.1679]\n",
    "\n",
    "transformation_kwargs:\n",
    "   crop_size: 153\n",
    "   normalize:\n",
    "      mean: [0.4049, 0.3090, 0.2831]\n",
    "      std: [0.2702, 0.1792, 0.1679]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the common_config.py file\n",
    "\n",
    " This file is responsiable for loading the dataset,model,train/val augmentations etc. And modifications made in the config files need to me made in this file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreTrained Model Weights \n",
    "\n",
    "\n",
    "Model weights for cluster = 6 are store in the Weights/Kenya_6CLS_153\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretext\n",
      "scan\n",
      "selflabel\n"
     ]
    }
   ],
   "source": [
    "!ls Weights/KenyaModel_6CLS_153/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The sacn model does a ok job at clustering differnt classes. A few  things to note here are. \n",
    "\n",
    "1. The data distribution is skewed. There were only 3 images for containing water, most of them belonged to shrubland.\n",
    "2. Some images contain multiple classes and makes it difficult fot the model to assign proper clusters. \n",
    "3. Cloudy Images : With cluster = 8 does the best job at clustering the cloudy images followed by 5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
